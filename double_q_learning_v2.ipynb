{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sutton and Barto Notebooks](https://github.com/seungjaeryanlee/sutton-barto-notebooks): Figure 6.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ryan Lee  \n",
    "저자: 이승재  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an implementation of Figure 6.5 in [Sutton and Barto's Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html).  \n",
    "이 노트북은 [Sutton and Barto의 Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) 책의 Figure 6.5를 구현한 결과입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 6.5](figure.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_all(list_):\n",
    "    \"\"\"\n",
    "    Returns all argmax of given list in a list. Different from np.argmax which returns first instance only.\n",
    "    주어진 list의 최대값들의 index들을 list 형태로 반환합니다. 첫 최대값의 index만 반환하는 np.argmax와는 다릅니다.\n",
    "    \"\"\"\n",
    "    return np.argwhere(list_ == np.max(list_)).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "class Action(IntEnum):\n",
    "    \"\"\"\n",
    "    Actions possible in state A.\n",
    "    상태 A에서 선택 가능한 행동들.\n",
    "    \"\"\"\n",
    "    LEFT = 0\n",
    "    RIGHT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Simple two-armed bandit environment shown in Sutton and Barto Figure 6.5.\n",
    "    Sutton and Barto Figure 6.5의 환경입니다.\n",
    "    \"\"\"\n",
    "    NUMBER_OF_ACTIONS = 10 # Number of actions possible in state B\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state = 'A'\n",
    "\n",
    "    def action_space(self):\n",
    "        \"\"\"\n",
    "        Return all possible actions for the current state.\n",
    "        현재 상태에서의 가능한 모든 행동을 반환합니다.\n",
    "        \"\"\"\n",
    "        if self.state == 'A':\n",
    "            return [Action.LEFT, Action.RIGHT]\n",
    "        elif self.state == 'B':\n",
    "            return range(self.NUMBER_OF_ACTIONS)\n",
    "        else:\n",
    "            return []\n",
    "\n",
    "    def is_terminal(self):\n",
    "        \"\"\"\n",
    "        Check is the current state is terminal.\n",
    "        현재 상태가 끝 상태인지 확인합니다.\n",
    "        \"\"\"\n",
    "        return self.state == 'X'\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset environment and return initial state.\n",
    "        환경을 초기화하고 첫 상태를 반환합니다.\n",
    "        \"\"\"\n",
    "        self.state = 'A'\n",
    "        return self.state\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Take a given action and return tuple (next_state, reward, done).\n",
    "        주어진 행동을 취하고 (다음 상태, 보상, 에피소드가 끝남)를 반환합니다.\n",
    "        \"\"\"\n",
    "        if self.state == 'A' and action == Action.LEFT:\n",
    "            self.state = 'B'\n",
    "            return ('B', 0, False)\n",
    "        elif self.state == 'A' and action == Action.RIGHT:\n",
    "            self.state = 'X'\n",
    "            return ('X', 0, True)\n",
    "        else:\n",
    "            self.state = 'X'\n",
    "            return ('X', np.random.normal(-0.1, 1), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    epsilon = 0.1\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 1\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initialize Q table and save environment.\n",
    "        Q 테이블을 초기화하고 주어진 환경을 보관합니다.\n",
    "        \"\"\"\n",
    "        self.q_table = {\n",
    "            'A': [0, 0],\n",
    "            'B': [0] * env.NUMBER_OF_ACTIONS\n",
    "        }\n",
    "        self.env = env\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns action based on Q table and epsilon-greedy policy.\n",
    "        엡실론 탐욕 정책과 Q테이블을 이용해 주어진 상태에서 할 행동을 반환합니다.\n",
    "        \"\"\"\n",
    "        best_actions = argmax_all(self.q_table[state])\n",
    "\n",
    "        if np.random.choice(['e', '1-e'], p=[self.epsilon, 1 - self.epsilon]) == 'e':\n",
    "            return np.random.choice(self.env.action_space())\n",
    "        else:\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q table via Q-learning.\n",
    "        Q러닝을 통해 Q 테이블을 업데이트합니다.\n",
    "        \"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.discount_factor * np.max(self.q_table[next_state])\n",
    "\n",
    "        self.q_table[state][action] = (1 - self.learning_rate) * self.q_table[state][action] + self.learning_rate * target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    epsilon = 0.1\n",
    "    learning_rate = 0.1\n",
    "    discount_factor = 1\n",
    "\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Initialize Q table and save environment.\n",
    "        Q 테이블을 초기화하고 주어진 환경을 보관합니다.\n",
    "        \"\"\"\n",
    "        self.q1 = {'A': [0, 0], 'B': [0] * env.NUMBER_OF_ACTIONS}\n",
    "        self.q2 = {'A': [0, 0], 'B': [0] * env.NUMBER_OF_ACTIONS}\n",
    "        self.q_table = [self.q1, self.q2]\n",
    "        self.env = env\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns action based on Q table and epsilon-greedy policy.\n",
    "        엡실론 탐욕 정책과 Q테이블을 이용해 주어진 상태에서 할 행동을 반환합니다.\n",
    "        \"\"\"\n",
    "        q_state = np.add(self.q1[state], self.q2[state])\n",
    "        best_actions = argmax_all(q_state)\n",
    "\n",
    "        if np.random.choice(['e', '1-e'], p=[self.epsilon, 1 - self.epsilon]) == 'e':\n",
    "            return np.random.choice(self.env.action_space())\n",
    "        else:\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q table via Double Q-learning.\n",
    "        더블 Q러닝을 통해 Q 테이블을 업데이트합니다.\n",
    "        \"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.discount_factor * self.q2[next_state][np.argmax(self.q1[next_state])]\n",
    "        \n",
    "        if np.random.choice(['q1', 'q2']) == 'q1':\n",
    "            self.q1[state][action] = (1 - self.learning_rate) * self.q1[state][action] + self.learning_rate * target\n",
    "        else:\n",
    "            self.q2[state][action] = (1 - self.learning_rate) * self.q2[state][action] + self.learning_rate * target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(Agent):\n",
    "    \"\"\"\n",
    "    Creates an agent with given Agent class and runs one episode. (Only for debugging)\n",
    "    주어진 Agent 클래스로 에이전트를 만들어서 에피소드를 1번 실행합니다. (디버깅 전용)\n",
    "    \"\"\"\n",
    "    env = Environment()\n",
    "    agent = Agent(env)\n",
    "\n",
    "    state = env.reset()\n",
    "    action = agent.get_action(state)\n",
    "    next_state, reward, done = env.step(action)\n",
    "\n",
    "    print('State      : ', state)\n",
    "    print('Action     : ', action)\n",
    "    print('Reward     : ', reward)\n",
    "    print('Next State : ', next_state)\n",
    "\n",
    "    agent.update_q(state, action, reward, next_state, done)\n",
    "    print(agent.q_table)\n",
    "\n",
    "    if not done:\n",
    "        state = next_state\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "\n",
    "        print('State      : ', state)\n",
    "        print('Action     : ', action)\n",
    "        print('Reward     : ', reward)\n",
    "        print('Next State : ', next_state)\n",
    "\n",
    "        agent.update_q(state, action, reward, next_state, done)\n",
    "        print(agent.q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State      :  A\n",
      "Action     :  0\n",
      "Reward     :  0\n",
      "Next State :  B\n",
      "[{'A': [0.0, 0], 'B': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'A': [0, 0], 'B': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}]\n",
      "State      :  B\n",
      "Action     :  6\n",
      "Reward     :  -0.8085614253005003\n",
      "Next State :  X\n",
      "[{'A': [0.0, 0], 'B': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}, {'A': [0, 0], 'B': [0, 0, 0, 0, 0, 0, -0.08085614253005004, 0, 0, 0]}]\n"
     ]
    }
   ],
   "source": [
    "run_episode(DoubleQLearningAgent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_trials(n_trials, n_episodes, Agent, Environment):\n",
    "    \"\"\"\n",
    "    Runs `n_trials` trials on given Agent and Environment. For each trial,\n",
    "    `n_episodes` episodes are run, and Q table is reset after each trial.\n",
    "    주어진 Agent와 Environment 클래스를 활용해서 트라이얼을 n_trial번 실행합니다. 각\n",
    "    트라이얼마다 에피소드를 n_episode번 실행하며, 매 트라이얼마다 Q 테이블을 초기화합니다.\n",
    "    \"\"\"\n",
    "    env = Environment()\n",
    "    left_selected_per_episode = [0] * n_episodes\n",
    "\n",
    "    for _ in range(n_trials):\n",
    "        agent = Agent(env)\n",
    "        for episode_i in range(n_episodes):\n",
    "            state = env.reset()\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update_q(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "\n",
    "            if action == Action.LEFT:\n",
    "                left_selected_per_episode[episode_i] += 1\n",
    "\n",
    "            if not done:\n",
    "                action = agent.get_action(state)\n",
    "                next_state, reward, done = env.step(action)\n",
    "                agent.update_q(state, action, reward, next_state, done)\n",
    "\n",
    "    # Calculate Percentage\n",
    "    left_select_percentages = np.divide(left_selected_per_episode, n_trials)\n",
    "    left_select_percentages[0] = 0.5\n",
    "    return left_select_percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TRIALS = 10000 #트라이얼 횟수: 오래 걸리니 100, 1000정도 추천\n",
    "NUM_EPISODES = 300 #트라이얼마다 행하는 에피소드 횟수\n",
    "\n",
    "p1 = run_trials(NUM_TRIALS, NUM_EPISODES, QLearningAgent, Environment)\n",
    "p2 = run_trials(NUM_TRIALS, NUM_EPISODES, DoubleQLearningAgent, Environment)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.plot(range(NUM_EPISODES), p1)\n",
    "plt.plot(range(NUM_EPISODES), p2)\n",
    "plt.plot(range(NUM_EPISODES), [DoubleQLearningAgent.epsilon / 2] * NUM_EPISODES)\n",
    "plt.legend(['Q-Learning', 'Double Q-Learning', 'Optimal'])\n",
    "# plt.savefig('result.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GUI(tk.Tk):\n",
    "    \"\"\"\n",
    "    Figure 6.5\n",
    "    \"\"\"\n",
    "    def __init__(self, agent, env):\n",
    "        self.episode_count = 0\n",
    "        self.agent = agent\n",
    "        self.env = env\n",
    "        tk.Tk.__init__(self)\n",
    "        self.canvas = self._build_canvas()\n",
    "        self._update_canvas(0)\n",
    "    \n",
    "    def _build_canvas(self):\n",
    "        width = 600\n",
    "        height = 350\n",
    "        diagram_y = 100\n",
    "        \n",
    "        r = 20\n",
    "        btm_margin = 50\n",
    "        t1_x, t1_y = (150, diagram_y)\n",
    "        b_x, b_y = (250, diagram_y)\n",
    "        a_x, a_y = (350, diagram_y)\n",
    "        t2_x, t2_y = (450, diagram_y)\n",
    "        \n",
    "        canvas = tk.Canvas(self, width=width, height=height, bg='white')\n",
    "        \n",
    "        canvas.create_oval(b_x-r, b_y-r, b_x+r, b_y+r)\n",
    "        canvas.create_text(b_x, b_y, text='B')\n",
    "        canvas.create_oval(a_x-r, a_y-r, a_x+r, a_y+r)\n",
    "        canvas.create_text(a_x, a_y, text='A')\n",
    "        canvas.create_rectangle(t1_x-r, t1_y-r, t1_x+r, t1_y+r, fill='#eee')\n",
    "        canvas.create_rectangle(t2_x-r, t2_y-r, t2_x+r, t2_y+r, fill='#eee')\n",
    "        canvas.create_line(b_x-r,b_y, t1_x+r, t1_y, tags=(\"line\",), arrow=\"last\")\n",
    "        self.arrow_left = canvas.create_line(a_x-r,a_y, b_x+r, b_y, tags=(\"line\",), arrow=\"last\")\n",
    "        self.arrow_right = canvas.create_line(a_x+r,a_y, t2_x-r, t2_y, tags=(\"line\",), arrow=\"last\")\n",
    "\n",
    "        # Info Text\n",
    "        self.text_episode_count = canvas.create_text(300, 25, text='Reward: 0')\n",
    "        self.text_total_reward = canvas.create_text(300, 50, text='Reward: 0')\n",
    "\n",
    "        # Q Table\n",
    "        canvas.create_text(t1_x/2, diagram_y+btm_margin, text='Q values')\n",
    "        self.text_left = canvas.create_text((b_x+a_x)/2, diagram_y+btm_margin, text='0.0')\n",
    "        self.text_right = canvas.create_text((a_x+t2_x)/2, diagram_y+btm_margin, text='0.0')\n",
    "        self.text_actions = []\n",
    "        for i in range(env.NUMBER_OF_ACTIONS):\n",
    "            self.text_actions.append(canvas.create_text((t1_x+b_x)/2, diagram_y+btm_margin+15*i, text='0.0'))\n",
    "\n",
    "        # Step Button\n",
    "        step_button = tk.Button(self, text=\"Evaluate\", command=self.run_episode, width=80, height=3)\n",
    "        canvas.create_window(300, 325, window=step_button)\n",
    "\n",
    "        canvas.pack(side=\"top\", fill=\"both\", expand=True)\n",
    "        \n",
    "        return canvas\n",
    "    \n",
    "    def run_episode(self):\n",
    "        state = self.env.reset()\n",
    "        action = self.agent.get_action(state)\n",
    "        next_state, reward, done = self.env.step(action)\n",
    "        self.agent.update_q(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward = reward\n",
    "\n",
    "        if not done:\n",
    "            action = self.agent.get_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "            self.agent.update_q(state, action, reward, next_state, done)\n",
    "            total_reward += reward\n",
    "        \n",
    "        self.episode_count += 1\n",
    "        self._update_canvas(total_reward)\n",
    "    \n",
    "    def _update_canvas(self, total_reward):\n",
    "        # Update Info\n",
    "        self.canvas.itemconfig(self.text_total_reward, text='Last Episode\\'s Total Reward: %f' % total_reward)\n",
    "        self.canvas.itemconfig(self.text_episode_count, text='Episode Count: %d' % self.episode_count)\n",
    "\n",
    "        # Update Q values\n",
    "        self.canvas.itemconfig(self.text_left, text='%3f' % self.agent.q_table['A'][Action.LEFT])\n",
    "        self.canvas.itemconfig(self.text_right, text='%3f' % self.agent.q_table['A'][Action.RIGHT])\n",
    "        for i in range(env.NUMBER_OF_ACTIONS):\n",
    "            self.canvas.itemconfig(self.text_actions[i], text='%3f' % self.agent.q_table['B'][i])\n",
    "        if self.agent.q_table['A'][Action.LEFT] > self.agent.q_table['A'][Action.RIGHT]:\n",
    "            self.canvas.itemconfig(self.arrow_left, fill='red')\n",
    "            self.canvas.itemconfig(self.arrow_right, fill='black')\n",
    "        else:\n",
    "            self.canvas.itemconfig(self.arrow_left, fill='black')\n",
    "            self.canvas.itemconfig(self.arrow_right, fill='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agent = QLearningAgent(env)\n",
    "gui = GUI(agent, env)\n",
    "gui.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpg]",
   "language": "python",
   "name": "conda-env-rlpg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
