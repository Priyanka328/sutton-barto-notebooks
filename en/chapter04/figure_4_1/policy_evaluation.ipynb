{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sutton and Barto Notebooks](https://github.com/seungjaeryanlee/sutton-barto-notebooks): Figure 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ModuAI](https://www.modu.ai)  \n",
    "Author: Seung Jae (Ryan) Lee  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure 4.1](figure_4_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import IntEnum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example 4.1](example_4_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(IntEnum):\n",
    "    \"\"\"\n",
    "    All possible actions in a 4x4 gridworld.\n",
    "    \"\"\"\n",
    "    UP = -4\n",
    "    DOWN = 4\n",
    "    RIGHT = 1\n",
    "    LEFT = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    The 4x4 gridworld shown in Example 4.1.\n",
    "    \"\"\"\n",
    "    state_space = [i for i in range(16)]\n",
    "    action_space = list(Action)\n",
    "\n",
    "    def peek(self, state, action):\n",
    "        \"\"\"\n",
    "        Returns the result of taking given action on the given state.\n",
    "        The result consists of next state and reward.\n",
    "        \"\"\"\n",
    "        if self.is_done(state):\n",
    "            return state, 0\n",
    "\n",
    "        if not ((state // 4 == 0 and action == Action.UP)\n",
    "            or (state // 4 == 3 and action == Action.DOWN)\n",
    "            or (state % 4 == 3 and action == Action.RIGHT)\n",
    "            or (state % 4 == 0 and action == Action.LEFT)):\n",
    "            state += action\n",
    "\n",
    "        return state, -1\n",
    "\n",
    "    def is_done(self, state):\n",
    "        \"\"\"\n",
    "        Returns True if given state is the terminal state.\n",
    "        \"\"\"\n",
    "        return state == 0 or state == 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pseudocode](policy_evaluation.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    \"\"\"\n",
    "    A random-policy agent.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.v = np.zeros(len(env.state_space))\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Update agent's state values based on random policy.\n",
    "        \"\"\"\n",
    "        new_v = np.zeros(len(self.env.state_space))\n",
    "        for state in self.env.state_space:\n",
    "            for action in self.env.action_space:\n",
    "                next_state, reward = self.env.peek(state, action)\n",
    "                new_v[state] += 1/4 * (reward + self.v[next_state])\n",
    "        self.v = new_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keymax(dict_):\n",
    "    \"\"\"\n",
    "    Get list of keys in a dictionary with maximum value.\n",
    "    \"\"\"\n",
    "    max_v = max(dict_.values())\n",
    "    return [key for key in dict_.keys() if dict_[key] == max_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GUI(tk.Tk):\n",
    "    \"\"\"\n",
    "    GUI for Figure 4.1\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, agent, n_steps=500):\n",
    "        self.env = env\n",
    "        self.agent = agent\n",
    "\n",
    "        self.step = 0\n",
    "        self.cache = []\n",
    "\n",
    "        # Precompute state values\n",
    "        for _ in range(n_steps):\n",
    "            self.cache.append(agent.v)\n",
    "            agent.update()\n",
    "\n",
    "        tk.Tk.__init__(self)\n",
    "        self.canvas = self._build_canvas()\n",
    "        self._update_canvas()\n",
    "\n",
    "    def _build_canvas(self):\n",
    "        \"\"\"\n",
    "        Builds a tkinter canvas.\n",
    "        \"\"\"\n",
    "        canvas_w, canvas_h = (480, 300) # Canvas Dimensions\n",
    "        g1_x, g1_y = (60, 100)          # Center of first square in grid 1\n",
    "        g2_x, g2_y = (300, 100)         # Center of first square in grid 2\n",
    "        r = 20                          # Radius of grid cell\n",
    "\n",
    "        canvas = tk.Canvas(self, width=canvas_w, height=canvas_h, bg='white')\n",
    "\n",
    "        # Info Text\n",
    "        self.text_step = canvas.create_text(canvas_w/2, 20)\n",
    "\n",
    "        # Grid 1\n",
    "        self.grid1 = []\n",
    "        for j in range(4):\n",
    "            for i in range(4):\n",
    "                x = g1_x + 2*r*i\n",
    "                y = g1_y + 2*r*j\n",
    "                canvas.create_rectangle(x-r, y-r, x+r, y+r)\n",
    "                self.grid1.append({'text': canvas.create_text(x, y)})\n",
    "\n",
    "        # Grid 2\n",
    "        self.grid2 = []\n",
    "        for j in range(4):\n",
    "            for i in range(4):\n",
    "                cell = {}\n",
    "\n",
    "                x = g2_x + 2*r*i\n",
    "                y = g2_y + 2*r*j\n",
    "                canvas.create_rectangle(x-r, y-r, x+r, y+r)\n",
    "                cell['text'] = canvas.create_text(x, y)\n",
    "\n",
    "                cell['arrows'] = {}\n",
    "                cell['arrows'][Action.UP] = canvas.create_line(\n",
    "                    x, y, x, y-r, arrow=\"last\")\n",
    "                cell['arrows'][Action.DOWN] = canvas.create_line(\n",
    "                    x, y, x, y+r, arrow=\"last\")\n",
    "                cell['arrows'][Action.LEFT] = canvas.create_line(\n",
    "                    x, y, x-r, y, arrow=\"last\")\n",
    "                cell['arrows'][Action.RIGHT] = canvas.create_line(\n",
    "                    x, y, x+r, y, arrow=\"last\")\n",
    "\n",
    "                self.grid2.append(cell)\n",
    "\n",
    "        # Back Button\n",
    "        back_button = tk.Button(\n",
    "            self, text=\"Back\", command=self.back_step, width=30, height=3)\n",
    "        canvas.create_window(canvas_w/4, canvas_h-25, window=back_button)\n",
    "\n",
    "        # Step Button\n",
    "        step_button = tk.Button(\n",
    "            self, text=\"Step\", command=self.run_step, width=30, height=3)\n",
    "        canvas.create_window(3*canvas_w/4, canvas_h-25, window=step_button)\n",
    "\n",
    "        canvas.pack(side=\"top\", fill=\"both\", expand=True)\n",
    "\n",
    "        return canvas\n",
    "\n",
    "    def run_step(self):\n",
    "        \"\"\"\n",
    "        Updates canvas with state values of the next step.\n",
    "        \"\"\"\n",
    "        if self.step >= len(self.cache) - 1:\n",
    "            return\n",
    "\n",
    "        self.step += 1\n",
    "        self._update_canvas()\n",
    "\n",
    "    def back_step(self):\n",
    "        \"\"\"\n",
    "        Updates canvas with state values of the previous step.\n",
    "        \"\"\"\n",
    "        if self.step <= 0:\n",
    "            return\n",
    "\n",
    "        self.step -= 1\n",
    "        self._update_canvas()\n",
    "\n",
    "    def _update_canvas(self):\n",
    "        \"\"\"\n",
    "        Updates the canvas.\n",
    "        \"\"\"\n",
    "        v = self.cache[self.step]\n",
    "\n",
    "        # Update Info\n",
    "        self.canvas.itemconfig(self.text_step, text='k = %d' % self.step)\n",
    "\n",
    "        # Update V\n",
    "        for state, value in enumerate(v):\n",
    "            self.canvas.itemconfig(self.grid1[state]['text'],\n",
    "                                   text='{:.1f}'.format(v[state]))\n",
    "\n",
    "        # Update greedy policy\n",
    "        for state, _ in enumerate(v):\n",
    "            for action in self.env.action_space:\n",
    "                self.canvas.itemconfig(self.grid2[state]['arrows'][action],\n",
    "                                       fill='white')\n",
    "        for state, value in enumerate(v):\n",
    "            if state == 0 or state == 15:\n",
    "                continue\n",
    "\n",
    "            nearby_values = {}\n",
    "            for action in self.env.action_space:\n",
    "                if state == 1 and action == Action.UP:\n",
    "                    next_state, reward = self.env.peek(state, action)\n",
    "                next_state, _ = self.env.peek(state, action)\n",
    "\n",
    "                nearby_values[action] = v[next_state]\n",
    "\n",
    "            best_actions = keymax(nearby_values)\n",
    "            for action in best_actions:\n",
    "                self.canvas.itemconfig(self.grid2[state]['arrows'][action],\n",
    "                                       fill='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agent = RandomAgent(env)\n",
    "gui = GUI(env, agent)\n",
    "gui.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:paper]",
   "language": "python",
   "name": "conda-env-paper-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
