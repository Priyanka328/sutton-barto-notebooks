{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Sutton and Barto Notebooks](https://github.com/seungjaeryanlee/sutton-barto-notebooks): Example 6.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Ryan Lee  \n",
    "저자: 이승재  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an implementation of Example 6.6 in [Sutton and Barto's Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html).  \n",
    "이 노트북은 [Sutton and Barto의 Reinforcement Learning: An Introduction](http://incompleteideas.net/book/the-book-2nd.html) 책의 Example 6.6을 구현한 결과입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Example 6.6](example_6_6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from enum import IntEnum\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax_all(list_):\n",
    "    \"\"\"\n",
    "    Returns all argmax of given list in a list. Different from np.argmax which returns first instance only.\n",
    "    주어진 list의 최대값들의 index들을 list 형태로 반환합니다. 첫 최대값의 index만 반환하는 np.argmax와는 다릅니다.\n",
    "    \"\"\"\n",
    "    return np.argwhere(list_ == np.max(list_)).flatten().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Action(IntEnum):\n",
    "    UP = 0\n",
    "    DOWN = 1\n",
    "    LEFT = 2\n",
    "    RIGHT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "\n",
    "    action_space = list(map(int, Action))\n",
    "\n",
    "    def __init__(self, width=12, height=4):\n",
    "        self.WIDTH = width\n",
    "        self.HEIGHT = height\n",
    "\n",
    "        self.state_space = [[x, y] for x in range(width) for y in range(height)]\n",
    "\n",
    "        self.state = [0, 0]\n",
    "\n",
    "    def _is_goal(self):\n",
    "        \"\"\"\n",
    "        Checks if current state is the goal state.\n",
    "        \"\"\"\n",
    "        return self.state == [11, 0]\n",
    "\n",
    "    def _is_cliff(self):\n",
    "        \"\"\"\n",
    "        Checks if current state is a cliff.\n",
    "        \"\"\"\n",
    "        return (self.state[0] in range(1, 11)) and (self.state[1] == 0)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets environment and returns initial state.\n",
    "        \"\"\"\n",
    "        self.state = [0, 0]\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Performs given action and returns next_state, reward, done.\n",
    "        \"\"\"\n",
    "        assert action in self.action_space\n",
    "\n",
    "        if action == Action.LEFT:\n",
    "            self.state[0] = max(self.state[0] - 1, 0)\n",
    "        elif action == Action.RIGHT:\n",
    "            self.state[0] = min(self.state[0] + 1, self.WIDTH - 1)\n",
    "        elif action == Action.DOWN:\n",
    "            self.state[1] = max(self.state[1] - 1, 0)\n",
    "        elif action == Action.UP:\n",
    "            self.state[1] = min(self.state[1] + 1, self.HEIGHT - 1)\n",
    "\n",
    "        if self._is_cliff():\n",
    "            reward = -100\n",
    "            self.state = [0, 0]\n",
    "        else:\n",
    "            reward = -1\n",
    "\n",
    "        done = self._is_goal()\n",
    "        return copy.copy(self.state), reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SarsaAgent:\n",
    "    def __init__(self, env, epsilon=0.1, learning_rate=0.1, discount_factor=1):\n",
    "        \"\"\"\n",
    "        Initialize Q table and save environment.\n",
    "        Q 테이블을 초기화하고 주어진 환경을 보관합니다.\n",
    "        \"\"\"\n",
    "        self.q_table = np.zeros((env.WIDTH, env.HEIGHT, len(env.action_space)), dtype=float)\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns action based on Q table and epsilon-greedy policy.\n",
    "        엡실론 탐욕 정책과 Q테이블을 이용해 주어진 상태에서 할 행동을 반환합니다.\n",
    "        \"\"\"\n",
    "        best_actions = argmax_all(self.q_table[state])\n",
    "\n",
    "        if np.random.choice(['e', '1-e'], p=[self.epsilon, 1 - self.epsilon]) == 'e':\n",
    "            return np.random.choice(self.env.action_space)\n",
    "        else:\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q table via SARSA.\n",
    "        SARSA를 통해 Q 테이블을 업데이트합니다.\n",
    "        \"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.discount_factor * self.q_table[next_state[0]][next_state[1]][self.get_action(next_state)]\n",
    "\n",
    "        self.q_table[state[0]][state[1]][action] = ((1 - self.learning_rate) * self.q_table[state[0]][state[1]][action]\n",
    "                                                 + self.learning_rate * target)\n",
    "\n",
    "    def plot_policy(self):\n",
    "        icons = ['↑', '↓', '←', '→']\n",
    "        for y in reversed(range(self.env.HEIGHT)):\n",
    "            for x in range(self.env.WIDTH):\n",
    "                if x in range(1, self.env.WIDTH - 1) and y == 0:\n",
    "                    print('O', end='')\n",
    "                else:\n",
    "                    print(icons[np.argmax(self.q_table[x][y])], end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env, epsilon=0.1, learning_rate=0.1, discount_factor=1):\n",
    "        \"\"\"\n",
    "        Initialize Q table and save environment.\n",
    "        Q 테이블을 초기화하고 주어진 환경을 보관합니다.\n",
    "        \"\"\"\n",
    "        self.q_table = np.zeros((env.WIDTH, env.HEIGHT, len(env.action_space)), dtype=float)\n",
    "        self.env = env\n",
    "        self.epsilon = epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Returns action based on Q table and epsilon-greedy policy.\n",
    "        엡실론 탐욕 정책과 Q테이블을 이용해 주어진 상태에서 할 행동을 반환합니다.\n",
    "        \"\"\"\n",
    "        best_actions = argmax_all(self.q_table[state])\n",
    "\n",
    "        if np.random.choice(['e', '1-e'], p=[self.epsilon, 1 - self.epsilon]) == 'e':\n",
    "            return np.random.choice(self.env.action_space)\n",
    "        else:\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def update_q(self, state, action, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Update Q table via Q-learning.\n",
    "        Q러닝을 통해 Q 테이블을 업데이트합니다.\n",
    "        \"\"\"\n",
    "        if done:\n",
    "            target = reward\n",
    "        else:\n",
    "            target = reward + self.discount_factor * np.max(self.q_table[next_state[0]][next_state[1]])\n",
    "\n",
    "        self.q_table[state[0]][state[1]][action] = ((1 - self.learning_rate) * self.q_table[state[0]][state[1]][action]\n",
    "                                                 + self.learning_rate * target)\n",
    "\n",
    "    def plot_policy(self):\n",
    "        icons = ['↑', '↓', '←', '→']\n",
    "        for y in reversed(range(self.env.HEIGHT)):\n",
    "            for x in range(self.env.WIDTH):\n",
    "                if x in range(1, self.env.WIDTH - 1) and y == 0:\n",
    "                    print('O', end='')\n",
    "                else:\n",
    "                    print(icons[np.argmax(self.q_table[x][y])], end='')\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def train_agent(env, agent, n_episodes=1):\n",
    "    \"\"\"\n",
    "    Train given agent in given environment 'n_episode' times.\n",
    "    \"\"\"\n",
    "    print('Episode {}/{}'.format(0, n_episodes))\n",
    "    agent.plot_policy()\n",
    "    \n",
    "    for i in range(n_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.get_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update_q(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "        clear_output(wait=True)\n",
    "        print('Episode {}/{}'.format(i+1, n_episodes))\n",
    "        agent.plot_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Environment()\n",
    "agent = QLearningAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10/10\n",
      "↓↓↓↓↓↓↓↓↓↓↓↓\n",
      "↓↓↓↓↓↓↓↓↓↓↓→\n",
      "→→→→→→→→→→→→\n",
      "↑OOOOOOOOOO↑\n"
     ]
    }
   ],
   "source": [
    "train_agent(env, agent, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:rlpg]",
   "language": "python",
   "name": "conda-env-rlpg-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
